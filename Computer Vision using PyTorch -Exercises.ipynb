{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu102\n",
      "0.9.1+cu102\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Import torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms \n",
    "from torchvision.transforms import ToTensor # covert image or np array to tensors\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check versions\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in ./.local/lib/python3.6/site-packages (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in ./.local/lib/python3.6/site-packages (from torchmetrics) (1.18.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/site-packages (from torchmetrics) (21.0)\n",
      "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.6/site-packages (from torchmetrics) (1.8.1)\n",
      "Requirement already satisfied: pyDeprecate==0.3.* in /usr/local/lib/python3.6/site-packages (from torchmetrics) (0.3.1)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/site-packages (from torch>=1.3.1->torchmetrics) (0.8)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.6/site-packages (from torch>=1.3.1->torchmetrics) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/site-packages (from packaging->torchmetrics) (2.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in ./.local/lib/python3.6/site-packages (0.19.0)\n",
      "Requirement already satisfied: numpy>=1.16.2 in ./.local/lib/python3.6/site-packages (from mlxtend) (1.18.5)\n",
      "Requirement already satisfied: setuptools in ./.local/lib/python3.6/site-packages (from mlxtend) (59.6.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in ./.local/lib/python3.6/site-packages (from mlxtend) (3.3.0)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.6/site-packages (from mlxtend) (0.17.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.6/site-packages (from mlxtend) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.6/site-packages (from mlxtend) (0.24.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.6/site-packages (from mlxtend) (0.25.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib>=3.0.0->mlxtend) (1.2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/site-packages (from matplotlib>=3.0.0->mlxtend) (8.3.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas>=0.24.2->mlxtend) (2019.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/site-packages (from scikit-learn>=0.20.3->mlxtend) (3.1.0)\n",
      "Requirement already satisfied: six in ./.local/lib/python3.6/site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device-agnostic code\n",
    "import torch \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the torchvision.datasets.MNIST() train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(\n",
    "                root = '.',\n",
    "                train = True,\n",
    "                download = True,\n",
    "                transform = transforms.ToTensor()) #convert the image or numpy data to tensors \n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "                root = '.',\n",
    "                train = False,\n",
    "                download = True,\n",
    "                transform = transforms.ToTensor()) #convert the image or numpy data to tensors) #donot convert class labels to tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding length of train and test dataset\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape of the data\n",
    "type(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the sample \n",
    "train_dataset[0] #so we can see data is in the form of image as array and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print image and label\n",
    "print(f\"Image: \\n {train_dataset[0][0]}\")\n",
    "print(f\"Label: \\n {train_dataset[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Image: \\n {train_dataset[1][0]}\")\n",
    "print(f\"Label: \\n {train_dataset[1][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_dataset[1][0]\n",
    "label = train_dataset[1][1]\n",
    "print(f\"Image shape:{image.shape} --> [Color channel first format], \\nlabel:{label} -->[no shape because its an integer]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.   Visualize at least 5 different samples of the MNIST training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the class names of train dataset\n",
    "class_names = train_dataset.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the index of the train dataset\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(5):\n",
    "    img = train_dataset[i][0]\n",
    "    print(f\"Image Shape : {image.shape}\")\n",
    "\n",
    "    img_squeeze=img.squeeze() #squeezing the image as the original image is in format color channel first. but matplotlib expects colorchannels last\n",
    "    print(f\"img_squeeze Shape : {img_squeeze.shape}\")\n",
    "\n",
    "    label = train_dataset[i][1]\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(img_squeeze) \n",
    "\n",
    "    plt.title(label)\n",
    "    plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting more images using randomness\n",
    "\n",
    "torch.manual_seed(42)\n",
    "fig = plt.figure(figsize =(9,9))\n",
    "rows,cols = 4,4\n",
    "for i in range(1, rows*cols+1):\n",
    "    random_idx = torch.randint(0, len(train_dataset), size =[1]).item()\n",
    "    img,label = train_dataset[random_idx]\n",
    "    fig.add_subplot(rows,cols,i);\n",
    "    plt.imshow(img.squeeze(), cmap ='gray')\n",
    "    plt.title(class_names[label]);\n",
    "    plt.axis(False); #removing grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Turn the MNIST train and test datasets into dataloaders using torch.utils.data.DataLoader, set the batch_size=32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn the dataset into dataloaders\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset = train_dataset,\n",
    "                             batch_size = 32,\n",
    "                             shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn the dataset into dataloaders\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "test_dataloader = DataLoader(dataset = test_dataset,\n",
    "                             batch_size = 32,\n",
    "                             shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of train_dataloader :{len(train_dataloader)},Length of test_dataloader :{len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Building CNN- TinyVGG model fitting on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the input and output shape before building the model\n",
    "\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "#next(iter(dataloader)) you can only access a single batch of data, this is more effecient than for loop if you want to view a single batch of data\n",
    "train_features_batch.shape, train_labels_batch.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_batch, test_labels_batch = next(iter(test_dataloader))\n",
    "#next(iter(dataloader)) you can only access a single batch of data, this is more effecient than for loop if you want to view a single batch of data\n",
    "test_features_batch.shape, test_labels_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The input image shape is 1 (inputshape denotes the number of color channels ). output shape is length of the class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a convolutional neural network\n",
    "class MNISTModelV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture copying TinyVGG from: \n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3, # how big is the square that's going over the image?\n",
    "                      stride=1, # default\n",
    "                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2) # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from? \n",
    "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "            nn.Linear(in_features=hidden_units*7*7, \n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.block_1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.block_2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "tinyvgg = MNISTModelV2(input_shape=1, hidden_units=10, output_shape=len(class_names)).to(device)\n",
    "tinyvgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the current state of the model\n",
    "tinyvgg.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train the model for 5 epochs on CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up accuracy, loss function and optimizer\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Truth labels for predictions.\n",
    "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "\n",
    "    Returns:\n",
    "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
    "    \"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=tinyvgg.parameters(), lr =0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to time our experiments\n",
    "\n",
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float,\n",
    "                     end: float, \n",
    "                     device: torch.device = None):\n",
    "    #Prints difference between start and end time\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#set the seed and start the timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "#set the number of epochs\n",
    "epochs = 5\n",
    "\n",
    "#creating training and test loop\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    \n",
    "    #set up train loss \n",
    "    train_loss = 0 #to calculate train loss step2\n",
    "    \n",
    "    #loop to loop through training batches\n",
    "    \n",
    "    for batch,(X,y) in enumerate(train_dataloader):\n",
    "        \n",
    "        1.#training model\n",
    "        tinyvgg.train()\n",
    "        \n",
    "        2. #forward pass\n",
    "        y_pred = tinyvgg(X)  \n",
    "        \n",
    "        3.#calculate the loss {per batch}\n",
    "        loss = loss_fn(y_pred,y)\n",
    "        train_loss += loss #accumulate the train loss\n",
    "        \n",
    "        4. #Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #5. Loss backward\n",
    "        loss.backward()\n",
    "        \n",
    "        #6. Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print out whats happening\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/ {len(train_dataloader.dataset)}\")\n",
    "        \n",
    "    \n",
    "    #Divide total train loss by length of train dataloader\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    #testing\n",
    "    test_loss, test_acc = 0,0\n",
    "    tinyvgg.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_dataloader:\n",
    "        \n",
    "            #1. forward pass\n",
    "            test_pred = tinyvgg(X_test)\n",
    "\n",
    "            #2. calculate loss (accumulately)\n",
    "            test_loss += loss_fn(test_pred, y_test)\n",
    "\n",
    "            #3. calculate accuracy\n",
    "            test_acc += accuracy_fn(y_true = y_test, y_pred = test_pred.argmax(dim=1))\n",
    "        \n",
    "        #calculate test loss average per batch\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        #calculate test accuracy average per batch\n",
    "        test_acc /= len(test_dataloader)\n",
    "        \n",
    "    #print out whats happening\n",
    "    print(f\"\\nTrain loss:{train_loss:.4f} | Test loss:{test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "\n",
    "#calculate training time\n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 =  print_train_time(start=train_time_start_on_cpu,\n",
    "                                            end = train_time_end_on_cpu,\n",
    "                                            device = str(next(tinyvgg.parameters()).device))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model on test dataset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def eval_model(model: torch.nn.Module,\n",
    "              data_loader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "            device=device):\n",
    "    \n",
    "    loss, acc = 0,0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X,y in tqdm(data_loader):\n",
    "            \n",
    "            #Make predictions\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            #Accumulate the loss and acc values per batch\n",
    "            loss += loss_fn(y_pred,y)\n",
    "            acc += accuracy_fn(y_true=y,\n",
    "                              y_pred=y_pred.argmax(dim=1))\n",
    "            # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "\n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "          \"model_loss\": loss.item(),\n",
    "          \"model_acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tinyvgg results on test dataset\n",
    "tinyvgg_results = eval_model(model=tinyvgg,\n",
    "                             data_loader=test_dataloader,\n",
    "                             loss_fn=loss_fn, \n",
    "                             accuracy_fn=accuracy_fn)\n",
    "tinyvgg_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.  Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_plot = 5\n",
    "for i in range(num_to_plot):\n",
    "    \n",
    "    #get images and labels from test data\n",
    "    img = test_dataset[i][0]\n",
    "    label= test_dataset[i][1]\n",
    "    \n",
    "    #make prediction on image\n",
    "    model_pred_logits = tinyvgg(img.unsqueeze(dim=0).to(device))\n",
    "    model_pred_probabilities= torch.softmax(model_pred_logits, dim=1)\n",
    "    model_pred_labels= torch.argmax(model_pred_probabilities, dim=1)\n",
    "    \n",
    "    #plot the image and prediction\n",
    "    plt.figure()\n",
    "    plt.imshow(img.squeeze(),cmap='gray')\n",
    "    plt.title(f\"Truth:{label}| Pred: {model_pred_labels.cpu().item()}\")\n",
    "    plt.axis(False);\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making prediction\n",
    "def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n",
    "    pred_probs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample in data:\n",
    "            # Prepare sample\n",
    "            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n",
    "\n",
    "            # Forward pass (model outputs raw logit)\n",
    "            pred_logit = model(sample)\n",
    "\n",
    "            # Get prediction probability (logit -> prediction probability)\n",
    "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n",
    "\n",
    "            # Get pred_prob off GPU for further calculations\n",
    "            pred_probs.append(pred_prob.cpu())\n",
    "            \n",
    "    # Stack the pred_probs to turn list into a tensor\n",
    "    return torch.stack(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking random samples from test_dataset\n",
    "import random\n",
    "random.seed(42)\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "for sample, label in random.sample(list(test_dataset), k=9):\n",
    "    test_samples.append(sample)\n",
    "    test_labels.append(label)\n",
    "\n",
    "# View the first test sample shape and label\n",
    "print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test samples\n",
    "pred_probs= make_predictions(model=tinyvgg, \n",
    "                             data=test_samples)\n",
    "\n",
    "# View first two prediction probabilities list\n",
    "pred_probs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the prediction probabilities into prediction labels by taking the argmax()\n",
    "pred_classes = pred_probs.argmax(dim=1)\n",
    "pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plt.figure(figsize=(9, 9))\n",
    "nrows = 3\n",
    "ncols = 3\n",
    "for i, sample in enumerate(test_samples):\n",
    "    # Create a subplot\n",
    "    plt.subplot(nrows, ncols, i+1)\n",
    "\n",
    "    # Plot the target image\n",
    "    plt.imshow(sample.squeeze(), cmap=\"gray\")\n",
    "\n",
    "    # Find the prediction label (in text form, e.g. \"Sandal\")\n",
    "    pred_label = class_names[pred_classes[i]]\n",
    "\n",
    "    # Get the truth label (in text form, e.g. \"T-shirt\")\n",
    "    truth_label = class_names[test_labels[i]] \n",
    "\n",
    "    # Create the title text of the plot\n",
    "    title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
    "  \n",
    "  # Check for equality and change title colour accordingly\n",
    "    if pred_label == truth_label:\n",
    "        plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n",
    "    else:\n",
    "        plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n",
    "    plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Plot a confusion matrix comparing your model's predictions to the truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the class names\n",
    "class_names = train_dataset.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Earlier we only predicted for few test samples,\n",
    "#inorder to plot confusion matrix we need to run prediction loop for entire test dataset\n",
    "# Import tqdm.auto\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "\n",
    "# 1. Make predictions with trained model\n",
    "y_preds = []\n",
    "tinyvgg.eval()\n",
    "with torch.inference_mode():\n",
    "    for batch,(X, y) in tqdm(enumerate(test_dataloader)):\n",
    "        # Send the data and targets to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Do the forward pass\n",
    "        y_logit = tinyvgg(X)\n",
    "        # Turn predictions from logits -> prediction probabilities -> prediction labels\n",
    "        y_pred = torch.softmax(y_logit.squeeze(), dim=0).argmax(dim=1)\n",
    "        # Put prediction on CPU for evaluation\n",
    "        y_preds.append(y_pred.cpu())\n",
    "\n",
    "# Concatenate list of predictions into a tensor\n",
    "# print(y_preds)\n",
    "y_pred_tensor = torch.cat(y_preds)\n",
    "y_pred_tensor\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.targets[:10],y_pred_tensor[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "# 2. Setup confusion instance and compare predictions to targets\n",
    "confmat = ConfusionMatrix(num_classes=len(class_names), task ='multiclass')\n",
    "confmat_tensor = confmat(preds=y_pred_tensor,\n",
    "                         target=test_dataset.targets)\n",
    "\n",
    "# 3. Plot the confusion matrix\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with numpy\n",
    "    class_names=class_names,\n",
    "    figsize=(10, 7)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save the model and make prediction on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#create model directory path\n",
    "\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents = True,\n",
    "                exist_ok = True)\n",
    "\n",
    "#Create model save\n",
    "MODEL_NAME = \"03_pytorch_computer_vision_EXERCISE.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "#save the model state dict\n",
    "print(f\"Saving model to : {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=tinyvgg.state_dict(),\n",
    "          f =MODEL_SAVE_PATH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new instance\n",
    "torch.manual_seed(42)\n",
    "\n",
    "loaded_tinyvgg = MNISTModelV2(input_shape=1,\n",
    "                             hidden_units = 10,\n",
    "                             output_shape=len(class_names))\n",
    "\n",
    "#load in the save state_dict()\n",
    "loaded_tinyvgg.state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
    "\n",
    "#send the model to the target device\n",
    "\n",
    "loaded_tinyvgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinyvgg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the loaded model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "loaded_tinyvgg_results = eval_model(\n",
    "                            model=loaded_tinyvgg,\n",
    "                             data_loader=test_dataloader,\n",
    "                             loss_fn=loss_fn, \n",
    "                             accuracy_fn=accuracy_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tinyvgg_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
